{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/vovanquangnbk/drowsy-train-face?scriptVersionId=144340782\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"!pip -q install facenet_pytorch","metadata":{"execution":{"iopub.status.busy":"2023-09-25T14:08:57.679611Z","iopub.execute_input":"2023-09-25T14:08:57.680388Z","iopub.status.idle":"2023-09-25T14:09:12.14035Z","shell.execute_reply.started":"2023-09-25T14:08:57.680353Z","shell.execute_reply":"2023-09-25T14:09:12.139135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from glob import glob\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nimport cv2\nfrom skimage import io\nimport torch\nfrom torch import nn\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport torchvision\nfrom torchvision import transforms\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nimport sklearn\nimport warnings\nimport joblib\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn import metrics\nimport warnings\nimport cv2\nimport pydicom\n\nfrom facenet_pytorch import MTCNN, InceptionResnetV1, extract_face","metadata":{"execution":{"iopub.status.busy":"2023-09-25T14:09:12.142655Z","iopub.execute_input":"2023-09-25T14:09:12.143009Z","iopub.status.idle":"2023-09-25T14:09:16.711011Z","shell.execute_reply.started":"2023-09-25T14:09:12.142973Z","shell.execute_reply":"2023-09-25T14:09:16.708928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG = {\n    'data_dir': '/kaggle/input/drowsy-cropfacevec-vggface',\n    'seed': 719,\n    'model_arch': 'LSTM',\n    'embedding_features': 2048, #512,\n    'train_all': False,\n    'epochs': 5,\n    'used_epochs': [2,3,4],\n    'train_bs': 8,\n    'valid_bs': 8,\n    'T_0': 10,\n    'lr': 1e-5,\n    'min_lr': 1e-6,\n    'weight_decay':1e-6,\n    'num_workers': 2,\n    'accum_iter': 2, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n    'verbose_step': 1,\n    'device': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n    'show_examples': False,\n}","metadata":{"execution":{"iopub.status.busy":"2023-09-25T14:09:16.716725Z","iopub.execute_input":"2023-09-25T14:09:16.721841Z","iopub.status.idle":"2023-09-25T14:09:16.764406Z","shell.execute_reply.started":"2023-09-25T14:09:16.721804Z","shell.execute_reply":"2023-09-25T14:09:16.763246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paths = os.path.join(CFG['data_dir'], \"*.csv\")\ndf = pd.concat(map(pd.read_csv, glob(paths)))\ndf = df.sample(frac=1)\ndf = df.reset_index(drop=True)\n\ntrain = df[df['fold'] != 'fold1']\ntest = df[df['fold'] == 'fold1']\nprint(train.shape)\nprint(test.shape)\n\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T14:09:16.768537Z","iopub.execute_input":"2023-09-25T14:09:16.769273Z","iopub.status.idle":"2023-09-25T14:09:16.865886Z","shell.execute_reply.started":"2023-09-25T14:09:16.769241Z","shell.execute_reply":"2023-09-25T14:09:16.865083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2023-09-25T14:09:16.869876Z","iopub.execute_input":"2023-09-25T14:09:16.872541Z","iopub.status.idle":"2023-09-25T14:09:16.88295Z","shell.execute_reply.started":"2023-09-25T14:09:16.872508Z","shell.execute_reply":"2023-09-25T14:09:16.881872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self,\n                 df,\n                 data_root=None,\n                 transforms=None,\n                 output_label=True,\n                 one_hot_label=False,\n                ):\n\n        super().__init__()\n        self.df = df.copy()\n        self.data_root = data_root\n        self.transforms = transforms\n        self.output_label = output_label\n        self.one_hot_label = one_hot_label\n\n        if output_label == True:\n            self.labels = self.df['label'].values\n            if one_hot_label is True:\n                self.labels = np.eye(self.df['label'].max()+1)[self.labels]\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, index: int):\n\n        # get labels\n        if self.output_label:\n            label = self.labels[index]\n\n        x_dir = os.path.join(\n            self.data_root, \n            self.df.iloc[index]['fold'],\n            self.df.iloc[index]['id'])\n        x = np.load(x_dir)\n        x = torch.from_numpy(x).to(torch.uint8)\n        x = x.permute((0,3,1,2))\n\n        if self.output_label == True:\n            return x, label\n        else:\n            return label\n\n# Test dataset\nif CFG['show_examples']:\n    dataset = MyDataset(train, CFG['data_dir'])\n    for i, (x, label) in enumerate(dataset):\n        print(x.shape, label)\n        if i > 5:\n            break","metadata":{"execution":{"iopub.status.busy":"2023-09-25T14:09:16.884299Z","iopub.execute_input":"2023-09-25T14:09:16.884641Z","iopub.status.idle":"2023-09-25T14:09:16.901939Z","shell.execute_reply.started":"2023-09-25T14:09:16.884595Z","shell.execute_reply":"2023-09-25T14:09:16.900937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PadSequence:\n    def __call__(self, batch):\n        # Let's assume that each element in \"batch\" is a tuple (data, label).\n        # Sort the batch in the descending order\n        sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n        # Get each sequence and pad it\n        sequences = [x[0] for x in sorted_batch]\n        sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n        # Also need to store the length of each sequence\n        # This is later needed in order to unpad the sequences\n        lengths = torch.LongTensor([len(x) for x in sequences])\n        # Don't forget to grab the labels of the *sorted* batch\n        labels = torch.LongTensor(list(map(lambda x: x[1], sorted_batch)))\n        return sequences_padded, labels\n\ndef prepare_dataloader(df, trn_idx, val_idx, train_all=CFG['train_all']):\n\n    from catalyst.data.sampler import BalanceClassSampler\n\n    train_ = df.loc[trn_idx,:].reset_index(drop=True)\n    valid_ = df.loc[val_idx,:].reset_index(drop=True)\n\n    train_ds = MyDataset(train_, data_root=CFG['data_dir'], output_label=True, one_hot_label=False)\n    valid_ds = MyDataset(valid_, data_root=CFG['data_dir'], output_label=True)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_ds,\n        batch_size=CFG['train_bs'],\n        pin_memory=False,\n        drop_last=False,\n        shuffle=True,\n        num_workers=CFG['num_workers'],\n        collate_fn = PadSequence(),\n#         sampler=BalanceClassSampler(labels=train_['label'].values, mode=\"downsampling\")\n    )\n    val_loader = torch.utils.data.DataLoader(\n        valid_ds,\n        batch_size=CFG['valid_bs'],\n        num_workers=CFG['num_workers'],\n        shuffle=False,\n        collate_fn = PadSequence(),\n        pin_memory=False,\n    )\n    return train_loader, val_loader\n\n# Test data loader\nif CFG['show_examples']:\n    dataset = MyDataset(train, CFG['data_dir'])\n    trn_idx = train[train['fold'] == 'fold4'].index.tolist()\n    val_idx = train[train['fold'] == 'fold3'].index.tolist()\n    train_loader, val_loader = prepare_dataloader(train, trn_idx, val_idx, train_all=CFG['train_all'])\n\n    print(len(train_loader))\n    for i, (x, label) in enumerate(train_loader):\n        x = x.to(CFG['device'])\n        label = label.to(CFG['device'])\n        print(x.shape, label.shape)\n        if i > 5:\n            break\n    print(len(val_loader))\n    for i, (x, label) in enumerate(val_loader):\n        x = x.to(CFG['device'])\n        label = label.to(CFG['device'])\n        print(x.shape, label.shape)\n        if i > 5:\n            break","metadata":{"execution":{"iopub.status.busy":"2023-09-25T14:09:16.906717Z","iopub.execute_input":"2023-09-25T14:09:16.909259Z","iopub.status.idle":"2023-09-25T14:09:16.93134Z","shell.execute_reply.started":"2023-09-25T14:09:16.909226Z","shell.execute_reply":"2023-09-25T14:09:16.930457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"### LSTM","metadata":{}},{"cell_type":"code","source":"class MyClassifier(nn.Module):\n    def __init__(self, input_size=512, hidden = 512):\n        super().__init__()\n        ## Inception Resnet\n        self.backbone = InceptionResnetV1(pretrained='vggface2')\n        \n        ## BiLSTM\n        self.hidden = hidden\n        self.input_size = input_size\n        self.lstm = nn.LSTM(self.input_size, self.hidden, batch_first=True)\n        self.fc1 = nn.Linear(self.hidden, 50)\n        self.selu = nn.SELU()\n        self.bn2 = nn.BatchNorm1d(50)\n        self.dropout = nn.Dropout(0.3)\n        self.fc2 = nn.Linear(50, 2)\n        self._reinitialize()\n\n    def _reinitialize(self):\n        \"\"\"\n        Tensorflow/Keras-like initialization\n        \"\"\"\n        for name, p in self.named_parameters():\n            if 'lstm' in name:\n                if 'weight_ih' in name:\n                    nn.init.xavier_uniform_(p.data)\n                elif 'weight_hh' in name:\n                    nn.init.orthogonal_(p.data)\n                elif 'bias_ih' in name:\n                    p.data.fill_(0)\n                    # Set forget-gate bias to 1\n                    n = p.size(0)\n                    p.data[(n // 4):(n // 2)].fill_(1)\n                elif 'bias_hh' in name:\n                    p.data.fill_(0)\n            elif 'fc' in name:\n                if 'weight' in name:\n                    nn.init.xavier_uniform_(p.data)\n                elif 'bias' in name:\n                    p.data.fill_(0)\n\n    def forward(self, x):\n        b,n,c,h,w = x.size()\n        x = x.reshape(-1,c,h,w)\n        x = x.float()\n        x = self.backbone(x)\n        x = x.reshape(-1,n,self.input_size)\n        x, _ = self.lstm(x)\n        x = x[:,-1,:]\n        x = self.dropout(x)\n        x = self.fc1(x)\n        x = self.selu(x)\n        x = self.fc2(x)\n\n        return x\n\n# Test model\nif CFG['show_examples']:\n    model = MyClassifier().to(CFG['device'])\n    dataset = MyDataset(df, CFG['data_dir'])\n    trn_idx = train[train['fold'] == 'fold2'].index.tolist()\n    val_idx = train[train['fold'] == 'fold3'].index.tolist()\n    train_loader, val_loader = prepare_dataloader(train, trn_idx, val_idx, train_all=CFG['train_all'])\n    loss = nn.BCEWithLogitsLoss().to(CFG['device'])\n\n    for i, (x, label) in enumerate(train_loader):\n        x = x.to(CFG['device']).byte()\n        preds = model(x).squeeze().to(CFG['device'])\n        label = label.float().to(CFG['device'])\n        print(preds.shape, label.shape)\n        print(loss(preds, label))\n        if i > 5:\n            break","metadata":{"execution":{"iopub.status.busy":"2023-09-25T14:09:16.93673Z","iopub.execute_input":"2023-09-25T14:09:16.939544Z","iopub.status.idle":"2023-09-25T14:09:16.962287Z","shell.execute_reply.started":"2023-09-25T14:09:16.939513Z","shell.execute_reply":"2023-09-25T14:09:16.961215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training function","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(epoch, model, loss_fn, optimizer, train_loader, device, scaler, scheduler=None, schd_batch_update=False, threshold=0.5):\n    model.train()\n\n    t = time.time()\n    running_loss = None\n    preds_all = []\n    y_all = []\n    threshold = threshold\n\n    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n    for step, (X, y) in pbar:\n        X = X.to(device).float()\n        y = y.to(device).long()\n\n        with autocast():\n            preds = model(X)\n            preds_all += [torch.argmax(preds, 1).detach().cpu().numpy()]\n            y_all += [y.detach().cpu().numpy()]\n\n            loss = loss_fn(preds, y)\n\n            scaler.scale(loss).backward()\n\n            if running_loss is None:\n                running_loss = loss.item()\n            else:\n                running_loss = running_loss * .99 + loss.item() * .01\n\n            if ((step + 1) %  CFG['accum_iter'] == 0) or ((step + 1) == len(train_loader)):\n                # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n\n                if scheduler is not None and schd_batch_update:\n                    scheduler.step()\n\n            if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(train_loader)):\n                description = f'epoch {epoch} loss: {running_loss:.4f}'\n\n                pbar.set_description(description)\n\n    if scheduler is not None and not schd_batch_update:\n        scheduler.step()\n    \n    preds_all = np.concatenate(preds_all)\n    y_all = np.concatenate(y_all)\n    print('train multi-class accuracy = {:.4f}'.format((preds_all==y_all).mean()))\n\ndef valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False, threshold=0.5):\n    model.eval()\n\n    t = time.time()\n    loss_sum = 0\n    sample_num = 0\n    preds_all = []\n    y_all = []\n\n    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n    for step, (X, y) in pbar:\n        X = X.to(device).float()\n        y = y.to(device).long()\n\n        preds = model(X)\n        preds_all += [torch.argmax(preds, 1).detach().cpu().numpy()]\n        y_all += [y.detach().cpu().numpy()]\n\n        loss = loss_fn(preds, y)\n\n        loss_sum += loss.item()*y.shape[0]\n        sample_num += y.shape[0]\n\n        if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(val_loader)):\n            description = f'epoch {epoch} loss: {loss_sum/sample_num:.4f}'\n            pbar.set_description(description)\n\n    preds_all = np.concatenate(preds_all)\n    y_all = np.concatenate(y_all)\n    print('validation multi-class accuracy = {:.4f}'.format((preds_all==y_all).mean()))\n\n    if scheduler is not None:\n        if schd_loss_update:\n            scheduler.step(loss_sum/sample_num)\n        else:\n            scheduler.step()\n            \ndef inference_one_epoch(model, data_loader, device):\n    model.eval()\n    preds_all = []\n\n    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n    for step, (X,y) in pbar:\n        X = X.to(device).float()\n        y = y.to(device).long()\n\n        preds = model(X)\n        preds_all += [torch.softmax(preds, 1).detach().cpu().numpy()]\n\n    preds_all = np.concatenate(preds_all)\n    \n    return preds_all","metadata":{"execution":{"iopub.status.busy":"2023-09-25T14:09:16.967223Z","iopub.execute_input":"2023-09-25T14:09:16.969887Z","iopub.status.idle":"2023-09-25T14:09:16.998482Z","shell.execute_reply.started":"2023-09-25T14:09:16.969857Z","shell.execute_reply":"2023-09-25T14:09:16.997554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main","metadata":{}},{"cell_type":"code","source":"if __name__ == '__main__':\n    seed_everything(CFG['seed'])\n    trn_idx = df[df['fold'] != f\"fold1\"].index.tolist()\n    test_idx = df[df['fold'] == f\"fold1\"].index.tolist()\n\n    print(len(trn_idx), len(test_idx))\n\n    train_loader, val_loader = prepare_dataloader(df, trn_idx, test_idx)\n\n    model = MyClassifier().to(CFG['device'])\n    scaler = GradScaler()\n    optimizer = torch.optim.Adam(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'])\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CFG['T_0'], T_mult=1, eta_min=CFG['min_lr'], last_epoch=-1)\n\n    loss_fn = nn.CrossEntropyLoss().to(CFG['device'])\n\n    for epoch in range(CFG['epochs']):\n        print(\"\\n\")\n        train_one_epoch(epoch, model, loss_fn, optimizer, train_loader, CFG['device'], scaler, scheduler=scheduler, schd_batch_update=False)\n\n        with torch.no_grad():\n            valid_one_epoch(epoch, model, loss_fn, val_loader, CFG['device'], scheduler=None, schd_loss_update=False, threshold=0.5)\n        if epoch >= 2:\n            torch.save(model.state_dict(), '{}_{}'.format(CFG['model_arch'], epoch))\n\n    del model, optimizer, train_loader, val_loader, scaler, scheduler\n    torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ckpt_path = '/kaggle/input/drowsy-ckpts'\nbest_model = MyClassifier().to(CFG['device'])\ntst_preds_all = []\n\ntest_ds = MyDataset(test, data_root=CFG['data_dir'], output_label=True)\ntst_loader = torch.utils.data.DataLoader(\n    test_ds,\n    batch_size=1,\n    num_workers=CFG['num_workers'],\n    shuffle=False,\n    collate_fn = PadSequence(),\n    pin_memory=False,\n)\n\nstart_time = time.time()\ntst_preds = []\n\nfor i, epoch in enumerate(CFG['used_epochs']):\n    best_model.load_state_dict(torch.load(ckpt_path+'/{}_{}'.format(CFG['model_arch'], epoch), map_location=torch.device(CFG['device'])))\n\n    with torch.no_grad():\n        tst_preds += [inference_one_epoch(best_model, tst_loader, CFG['device'])]\n\ntst_preds_all += [np.mean(tst_preds, axis=0)]\n\ndel best_model\ntorch.cuda.empty_cache()\n\ntst_preds_all = np.mean(tst_preds_all, axis=0)\ntst_preds_all = np.argmax(tst_preds_all, axis=1)\n\ntest['preds'] = tst_preds_all\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","metadata":{"execution":{"iopub.status.busy":"2023-09-25T14:09:17.006188Z","iopub.execute_input":"2023-09-25T14:09:17.008707Z","iopub.status.idle":"2023-09-25T14:10:32.088638Z","shell.execute_reply.started":"2023-09-25T14:09:17.008676Z","shell.execute_reply":"2023-09-25T14:10:32.087535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n\nprint(\"Multi-class accuracy: \", accuracy_score(test[\"label\"], test[\"preds\"]))\nprint(\"F1-score: \", f1_score(test[\"label\"], test[\"preds\"]))\nprint(\"Precision: \", precision_score(test[\"label\"], test[\"preds\"]))\nprint(\"Recall: \", recall_score(test[\"label\"], test[\"preds\"]))","metadata":{"execution":{"iopub.status.busy":"2023-09-25T14:10:32.090731Z","iopub.execute_input":"2023-09-25T14:10:32.091442Z","iopub.status.idle":"2023-09-25T14:10:32.110977Z","shell.execute_reply.started":"2023-09-25T14:10:32.091402Z","shell.execute_reply":"2023-09-25T14:10:32.110113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\ny_true = test[\"label\"]\ny_pred = test[\"preds\"]\ncf_mt = confusion_matrix(y_true, y_pred)\nsns.heatmap(cf_mt, annot=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T14:10:32.113852Z","iopub.execute_input":"2023-09-25T14:10:32.114139Z","iopub.status.idle":"2023-09-25T14:10:32.593832Z","shell.execute_reply.started":"2023-09-25T14:10:32.114115Z","shell.execute_reply":"2023-09-25T14:10:32.592906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print wrong predictions\nwrong_ids = y_true != y_pred\nwrong_preds = test[wrong_ids]\nwrong_preds.sample(20)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T14:11:07.568418Z","iopub.execute_input":"2023-09-25T14:11:07.568795Z","iopub.status.idle":"2023-09-25T14:11:07.584011Z","shell.execute_reply.started":"2023-09-25T14:11:07.568764Z","shell.execute_reply":"2023-09-25T14:11:07.583007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print correct predictions\ncorrect_ids = y_true == y_pred\ncorrect_preds = test[correct_ids]\ncorrect_preds.sample(20)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T14:11:10.96034Z","iopub.execute_input":"2023-09-25T14:11:10.960807Z","iopub.status.idle":"2023-09-25T14:11:10.982669Z","shell.execute_reply.started":"2023-09-25T14:11:10.960748Z","shell.execute_reply":"2023-09-25T14:11:10.981437Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/vovanquangnbk/drowsy-train-eyelandmarks?scriptVersionId=144340922\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"!pip -q install mediapipe\n!pip -q install torchsummary","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:25:50.852499Z","iopub.execute_input":"2023-09-25T12:25:50.853589Z","iopub.status.idle":"2023-09-25T12:26:23.314093Z","shell.execute_reply.started":"2023-09-25T12:25:50.853536Z","shell.execute_reply":"2023-09-25T12:26:23.312646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from glob import glob\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nimport cv2\nfrom skimage import io\nimport torch\nfrom torch import nn\nimport os\nimport itertools\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport torchvision\nfrom torchvision import transforms\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nimport sklearn\nimport warnings\nimport joblib\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn import metrics\nimport warnings\nimport cv2\nimport pydicom\nimport mediapipe as mp","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:26:23.316143Z","iopub.execute_input":"2023-09-25T12:26:23.316467Z","iopub.status.idle":"2023-09-25T12:26:39.598597Z","shell.execute_reply.started":"2023-09-25T12:26:23.316436Z","shell.execute_reply":"2023-09-25T12:26:39.597251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG = {\n    'data_dir': '/kaggle/input/drowsy-eye-keypoints',\n    'seed': 719,\n    'model_arch': 'CNN',\n    'train_all': False,\n    'epochs': 5,\n    'used_epochs':[2,3,4],\n    'train_bs': 8,\n    'valid_bs': 8,\n    'T_0': 10,\n    'lr': 1e-4,\n    'min_lr': 1e-6,\n    'weight_decay':1e-6,\n    'num_workers': 2,\n    'accum_iter': 2, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n    'verbose_step': 1,\n    'device': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n    'show_examples': True,\n}","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:26:39.600338Z","iopub.execute_input":"2023-09-25T12:26:39.601635Z","iopub.status.idle":"2023-09-25T12:26:39.611751Z","shell.execute_reply.started":"2023-09-25T12:26:39.601588Z","shell.execute_reply":"2023-09-25T12:26:39.610498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paths = os.path.join(CFG['data_dir'], \"*.csv\")\ndf = pd.concat(map(pd.read_csv, glob(paths)))\ndf = df.sample(frac=1)\ndf = df.reset_index(drop=True)\n\ntrain = df[df['fold'] != 'fold1']\ntest = df[df['fold'] == 'fold1']\n\nif CFG['show_examples']:\n    print(train.shape)\n    print(test.shape)\n    print(test.head())","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:26:39.615618Z","iopub.execute_input":"2023-09-25T12:26:39.61608Z","iopub.status.idle":"2023-09-25T12:26:39.686176Z","shell.execute_reply.started":"2023-09-25T12:26:39.616037Z","shell.execute_reply":"2023-09-25T12:26:39.685071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:26:39.687638Z","iopub.execute_input":"2023-09-25T12:26:39.6883Z","iopub.status.idle":"2023-09-25T12:26:39.696474Z","shell.execute_reply.started":"2023-09-25T12:26:39.688259Z","shell.execute_reply":"2023-09-25T12:26:39.695269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"# Eye Landmark indices\nmp_face_mesh = mp.solutions.face_mesh\n\nLEFT_EYE_INDEXES = list(set(itertools.chain(*mp_face_mesh.FACEMESH_LEFT_EYE)))\nRIGHT_EYE_INDEXES = list(set(itertools.chain(*mp_face_mesh.FACEMESH_RIGHT_EYE)))             \n\n\ndef indexes(list_obj, element_list):\n    return [list_obj.index(e) for e in element_list]\n\nLEFT_EYE_UPPER = indexes(LEFT_EYE_INDEXES, [398, 384, 385, 386, 387, 388, 466])\nLEFT_EYE_LOWER = indexes(LEFT_EYE_INDEXES, [382, 381, 380, 374, 373, 390, 249])\nLEFT_EYE_LEFT = LEFT_EYE_INDEXES.index(362)\nLEFT_EYE_RIGHT = LEFT_EYE_INDEXES.index(263)\nif CFG['show_examples']:\n    print(LEFT_EYE_UPPER)\n    print(LEFT_EYE_LOWER)\n    print(LEFT_EYE_LEFT)\n    print(LEFT_EYE_RIGHT)\n\nRIGHT_EYE_UPPER = indexes(RIGHT_EYE_INDEXES, [246, 161, 160, 159, 158, 157, 173])\nRIGHT_EYE_LOWER = indexes(RIGHT_EYE_INDEXES, [7, 163, 144, 145, 153, 154, 155])\nRIGHT_EYE_LEFT = RIGHT_EYE_INDEXES.index(33)\nRIGHT_EYE_RIGHT = RIGHT_EYE_INDEXES.index(133)\nif CFG['show_examples']:\n    print(LEFT_EYE_UPPER)\n    print(LEFT_EYE_LOWER)\n    print(LEFT_EYE_LEFT)\n    print(LEFT_EYE_RIGHT)\n\n# Source: https://raw.githubusercontent.com/tensorflow/tfjs-models/master/face-landmarks-detection/mesh_map.jpg","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:26:39.698166Z","iopub.execute_input":"2023-09-25T12:26:39.699273Z","iopub.status.idle":"2023-09-25T12:26:39.713917Z","shell.execute_reply.started":"2023-09-25T12:26:39.699207Z","shell.execute_reply":"2023-09-25T12:26:39.712729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self,\n                 df,\n                 data_root=None,\n                 transforms=None,\n                 output_label=True,\n                 one_hot_label=False,\n                ):\n\n        super().__init__()\n        self.df = df.copy()\n        self.data_root = data_root\n        self.transforms = transforms\n        self.output_label = output_label\n        self.one_hot_label = one_hot_label\n\n        if output_label == True:\n            self.labels = self.df['label'].values\n            if one_hot_label is True:\n                self.labels = np.eye(self.df['label'].max()+1)[self.labels]\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, index: int):\n\n        # get labels\n        if self.output_label:\n            label = self.labels[index]\n\n        x_dir = os.path.join(\n            self.data_root, \n            self.df.iloc[index]['fold'],\n            self.df.iloc[index]['id'])\n        x = np.load(x_dir)\n        upper = x[:,LEFT_EYE_UPPER,:][:,:,[1,3]]\n        lower = x[:,LEFT_EYE_LOWER,:][:,:,[1,3]]\n        eye_corner = x[:,LEFT_EYE_LEFT, [1,3]] - x[:,LEFT_EYE_RIGHT, [1,3]]\n        \n        EAR = np.sum(np.abs(upper - lower), axis=1) / (7*np.abs(eye_corner) + 1e-10)\n        \n        if EAR.shape[0] >= 100:\n            EAR = EAR[:100,:]\n        else:\n            length = EAR.shape[0]\n            EAR = np.pad(EAR, ((0,100-length),(0,0)), mode='constant', constant_values=-1)\n            \n        EAR = torch.from_numpy(EAR)\n        EAR = EAR.permute(1,0)\n        EAR = F.normalize(EAR)\n        \n        if self.output_label == True:\n            return EAR, label\n        else:\n            return EAR\n\n# Test dataset\nif CFG['show_examples']:\n    dataset = MyDataset(train, CFG['data_dir'])\n    for i, (EAR, label) in enumerate(dataset):\n        print(EAR.shape, label, torch.isnan(EAR).sum())\n        if i > 10:\n            break","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:26:39.715843Z","iopub.execute_input":"2023-09-25T12:26:39.716602Z","iopub.status.idle":"2023-09-25T12:26:39.928365Z","shell.execute_reply.started":"2023-09-25T12:26:39.716538Z","shell.execute_reply":"2023-09-25T12:26:39.927231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_dataloader(df, trn_idx, val_idx, train_all=CFG['train_all']):\n\n    from catalyst.data.sampler import BalanceClassSampler\n\n    train_ = df.loc[trn_idx,:].reset_index(drop=True)\n    valid_ = df.loc[val_idx,:].reset_index(drop=True)\n\n    train_ds = MyDataset(train_, data_root=CFG['data_dir'], output_label=True, one_hot_label=False)\n    valid_ds = MyDataset(valid_, data_root=CFG['data_dir'], output_label=True)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_ds,\n        batch_size=CFG['train_bs'],\n        pin_memory=False,\n        drop_last=False,\n        shuffle=True,\n        num_workers=CFG['num_workers'],\n#         sampler=BalanceClassSampler(labels=train_['label'].values, mode=\"downsampling\")\n    )\n    val_loader = torch.utils.data.DataLoader(\n        valid_ds,\n        batch_size=CFG['valid_bs'],\n        num_workers=CFG['num_workers'],\n        shuffle=False,\n        pin_memory=False,\n    )\n    return train_loader, val_loader\n\n# Test data loader\nif CFG['show_examples']:\n    dataset = MyDataset(train, CFG['data_dir'])\n    trn_idx = train[train['fold'] == 'fold4'].index.tolist()\n    val_idx = train[train['fold'] == 'fold3'].index.tolist()\n    train_loader, val_loader = prepare_dataloader(train, trn_idx, val_idx, train_all=CFG['train_all'])\n\n    print(len(train_loader))\n    for i, (x, label) in enumerate(train_loader):\n        x = x.to(CFG['device'])\n        label = label.to(CFG['device'])\n        print(x.shape, label.shape)\n        if i > 5:\n            break\n    print(len(val_loader))\n    for i, (x, label) in enumerate(val_loader):\n        x = x.to(CFG['device'])\n        label = label.to(CFG['device'])\n        print(x.shape, label.shape)\n        if i > 5:\n            break","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:26:39.930037Z","iopub.execute_input":"2023-09-25T12:26:39.931084Z","iopub.status.idle":"2023-09-25T12:26:42.885209Z","shell.execute_reply.started":"2023-09-25T12:26:39.931042Z","shell.execute_reply":"2023-09-25T12:26:42.883538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"class MyClassifier(nn.Module):\n    def __init__(self, num_classes=2, out_channels=[20,20]):\n        super(MyClassifier, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=2, out_channels=out_channels[0],  kernel_size = 10)\n        self.conv2 = nn.Conv1d(in_channels=out_channels[0], out_channels=out_channels[1], kernel_size=5)\n        self.pool = nn.MaxPool1d(2)\n        self.batchnorm1 = nn.BatchNorm1d(out_channels[0])\n        self.batchnorm2 = nn.BatchNorm1d(out_channels[1])\n        self.drop = nn.Dropout1d(p=0.2)\n        self.fc = nn.Linear(20*20, out_features=num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.pool(self.conv1(x))) \n        x = self.batchnorm1(x)\n        x = F.relu(self.pool(self.conv2(x)))\n        x = self.batchnorm2(x)\n        x = F.dropout(self.drop(x), training=self.training)\n        x = x.view(-1, 20*20)\n        x = self.fc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:26:42.887345Z","iopub.execute_input":"2023-09-25T12:26:42.887834Z","iopub.status.idle":"2023-09-25T12:26:42.901134Z","shell.execute_reply.started":"2023-09-25T12:26:42.887784Z","shell.execute_reply":"2023-09-25T12:26:42.899281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary\nmodel = MyClassifier()\nsummary(model, (2, 100))","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:26:42.906409Z","iopub.execute_input":"2023-09-25T12:26:42.906905Z","iopub.status.idle":"2023-09-25T12:26:43.207433Z","shell.execute_reply.started":"2023-09-25T12:26:42.906842Z","shell.execute_reply":"2023-09-25T12:26:43.206183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Traning API","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(epoch, model, loss_fn, optimizer, train_loader, device, scaler, scheduler=None, schd_batch_update=False, threshold=0.5):\n    model.train()\n\n    t = time.time()\n    running_loss = None\n    preds_all = []\n    y_all = []\n    threshold = threshold\n\n    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n    for step, (X, y) in pbar:\n        X = X.to(device).float()\n        y = y.to(device).long()\n\n        with autocast():\n            preds = model(X)\n            preds_all += [torch.argmax(preds, 1).detach().cpu().numpy()]\n            y_all += [y.detach().cpu().numpy()]\n\n            loss = loss_fn(preds, y)\n\n            scaler.scale(loss).backward()\n\n            if running_loss is None:\n                running_loss = loss.item()\n            else:\n                running_loss = running_loss * .99 + loss.item() * .01\n\n            if ((step + 1) %  CFG['accum_iter'] == 0) or ((step + 1) == len(train_loader)):\n                # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n\n                if scheduler is not None and schd_batch_update:\n                    scheduler.step()\n\n            if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(train_loader)):\n                description = f'epoch {epoch} loss: {running_loss:.4f}'\n\n                pbar.set_description(description)\n\n    if scheduler is not None and not schd_batch_update:\n        scheduler.step()\n    \n    preds_all = np.concatenate(preds_all)\n    y_all = np.concatenate(y_all)\n    print('train multi-class accuracy = {:.4f}'.format((preds_all==y_all).mean()))\n\ndef valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False, threshold=0.5):\n    model.eval()\n\n    t = time.time()\n    loss_sum = 0\n    sample_num = 0\n    preds_all = []\n    y_all = []\n\n    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n    for step, (X, y) in pbar:\n        X = X.to(device).float()\n        y = y.to(device).long()\n\n        preds = model(X)\n        preds_all += [torch.argmax(preds, 1).detach().cpu().numpy()]\n        y_all += [y.detach().cpu().numpy()]\n\n        loss = loss_fn(preds, y)\n\n        loss_sum += loss.item()*y.shape[0]\n        sample_num += y.shape[0]\n\n        if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(val_loader)):\n            description = f'epoch {epoch} loss: {loss_sum/sample_num:.4f}'\n            pbar.set_description(description)\n\n    preds_all = np.concatenate(preds_all)\n    y_all = np.concatenate(y_all)\n    print('validation multi-class accuracy = {:.4f}'.format((preds_all==y_all).mean()))\n\n    if scheduler is not None:\n        if schd_loss_update:\n            scheduler.step(loss_sum/sample_num)\n        else:\n            scheduler.step()\n            \ndef inference_one_epoch(model, data_loader, device):\n    model.eval()\n    preds_all = []\n\n    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n    for step, X in pbar:\n        X = X.to(device).float()\n\n        preds = model(X)\n        preds_all += [torch.softmax(preds, 1).detach().cpu().numpy()]\n\n    preds_all = np.concatenate(preds_all)\n    \n    return preds_all","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:26:43.209275Z","iopub.execute_input":"2023-09-25T12:26:43.209817Z","iopub.status.idle":"2023-09-25T12:26:43.234488Z","shell.execute_reply.started":"2023-09-25T12:26:43.209778Z","shell.execute_reply":"2023-09-25T12:26:43.233262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main","metadata":{}},{"cell_type":"code","source":"if __name__ == '__main__':\n    seed_everything(CFG['seed'])\n    folds = []\n    \n    trn_idx = df[df['fold'] != f\"fold1\"].index.tolist()\n    test_idx = df[df['fold'] == f\"fold1\"].index.tolist()\n\n    print(len(trn_idx), len(test_idx))\n\n    train_loader, val_loader = prepare_dataloader(df, trn_idx, test_idx)\n\n    model = MyClassifier().to(CFG['device'])\n    scaler = GradScaler()\n    optimizer = torch.optim.Adam(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'])\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CFG['T_0'], T_mult=1, eta_min=CFG['min_lr'], last_epoch=-1)\n\n    loss_fn = nn.CrossEntropyLoss().to(CFG['device'])\n\n    for epoch in range(CFG['epochs']):\n        print(\"\\n\")\n        train_one_epoch(epoch, model, loss_fn, optimizer, train_loader, CFG['device'], scaler, scheduler=scheduler, schd_batch_update=False)\n\n        with torch.no_grad():\n            valid_one_epoch(epoch, model, loss_fn, val_loader, CFG['device'], scheduler=None, schd_loss_update=False, threshold=0.5)\n        if epoch >= 2:\n            torch.save(model.state_dict(), '{}_{}'.format(CFG['model_arch'], epoch))\n\n    del model, optimizer, train_loader, val_loader, scaler, scheduler\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:26:43.236126Z","iopub.execute_input":"2023-09-25T12:26:43.236467Z","iopub.status.idle":"2023-09-25T12:27:01.61796Z","shell.execute_reply.started":"2023-09-25T12:26:43.236438Z","shell.execute_reply":"2023-09-25T12:27:01.616811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"best_model = MyClassifier().to(CFG['device'])\ntst_preds_all = []\nthreshold = 0.085\n\ntest_ds = MyDataset(test, data_root=CFG['data_dir'], output_label=False)\ntst_loader = torch.utils.data.DataLoader(\n    test_ds,\n    batch_size=CFG['valid_bs'],\n    num_workers=CFG['num_workers'],\n    shuffle=False,\n    pin_memory=False,\n)\n\ntst_preds = []\n\nstart_time = time.time()\nfor i, epoch in enumerate(CFG['used_epochs']):\n    best_model.load_state_dict(torch.load('{}_{}'.format(CFG['model_arch'], epoch), map_location=torch.device(CFG['device'])))\n\n    with torch.no_grad():\n        tst_preds += [inference_one_epoch(best_model, tst_loader, CFG['device'])]\n\ntst_preds_all += [np.mean(tst_preds, axis=0)]\n\ndel best_model\ntorch.cuda.empty_cache()\n\ntst_preds_all = np.mean(tst_preds_all, axis=0)\ntst_preds_all = np.argmax(tst_preds_all, axis=1)\n\ntest['preds'] = tst_preds_all\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:27:01.619635Z","iopub.execute_input":"2023-09-25T12:27:01.619987Z","iopub.status.idle":"2023-09-25T12:27:04.02384Z","shell.execute_reply.started":"2023-09-25T12:27:01.619954Z","shell.execute_reply":"2023-09-25T12:27:04.022833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n\nprint(\"Multi-class accuracy: \", accuracy_score(test[\"label\"], test[\"preds\"]))\nprint(\"F1-score: \", f1_score(test[\"label\"], test[\"preds\"]))\nprint(\"Precision: \", precision_score(test[\"label\"], test[\"preds\"]))\nprint(\"Recall: \", recall_score(test[\"label\"], test[\"preds\"]))","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:27:04.026196Z","iopub.execute_input":"2023-09-25T12:27:04.026834Z","iopub.status.idle":"2023-09-25T12:27:04.050794Z","shell.execute_reply.started":"2023-09-25T12:27:04.026794Z","shell.execute_reply":"2023-09-25T12:27:04.049671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\ny_true = test[\"label\"]\ny_pred = test[\"preds\"]\ncf_mt = confusion_matrix(y_true, y_pred)\nsns.heatmap(cf_mt, annot=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:27:04.052375Z","iopub.execute_input":"2023-09-25T12:27:04.052839Z","iopub.status.idle":"2023-09-25T12:27:04.665412Z","shell.execute_reply.started":"2023-09-25T12:27:04.052796Z","shell.execute_reply":"2023-09-25T12:27:04.664576Z"},"trusted":true},"execution_count":null,"outputs":[]}]}